<html lang="en">

<head>
  <link rel="icon" type="image/png" href="/misc/favicon-32x32.png" sizes="32x32" />
  <link rel="icon" type="image/png" href="/misc/favicon-16x16.png" sizes="16x16" />

  <link href="https://fonts.googleapis.com/css?family=Lato:300,400,700" rel="stylesheet">
  <meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
  <title>go-seq | James Briggs' Blog</title>
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <script src="/js/jquery.min.js"></script>
  <script src="/js/bootstrap.min.js"></script>
  <script src="/js/header.js"></script>
  <script src="/js/toc.js"></script>
  <link href="/css/bootstrap.min.css" rel="stylesheet">
  <link href="/css/theme.css" rel="stylesheet">
  <link href="/css/font-awesome/css/font-awesome.min.css" rel="stylesheet">
  <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
  <script type="text/x-mathjax-config">
	MathJax.Hub.Config({
		tex2jax: {
			inlineMath: [ ['$','$'], ["\\(","\\)"] ],
			displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
			processEscapes: false
		},
		"HTML-CSS": {
			preferredFont: "TeX",
		}
	})
  </script>
  <link href="/css/syntax.css" rel="stylesheet">
</head>

<body>

  

  


  <nav class="navbar navbar-inverse">
    <div class="container">
      <div class="navbar-header">
        <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand" href="/">go-seq</a>
      </div>
      <div class="collapse navbar-collapse">
        <ul class="nav navbar-nav">
          <li><a href="/">/home</a></li>
          <li><a href="/archive.html">/archive</a></li>
          <li><a href="/tags.html">/tags</a></li>
          <li><a href="/about.html">/about</a></li>
          <li><a href="/feed.xml">/RSS</a></li>
        </ul>
      </div>
    </div>
  </nav>


<div class="wrapper">
  <div class="content">
    <div class="container container-center">
      <div class="row">
        <div class="col-md-12">
          <div class="well">
            

<h1 class="post-title">
	<a href="/2016/12/fleuronbovw">
		SIFTing Images
	</a>
</h1>

<div class="post-meta">
	<div class="post-time">
		<i class="fa fa-calendar"></i>
		<time>30 Dec 2016</time>
	</div>
	<ul>
		
		<li><a href="/tag/python">python</a></li>
		
		<li><a href="/tag/machine-learning">machine-learning</a></li>
		
		<li><a href="/tag/computer-vision">computer-vision</a></li>
		
		<li><a href="/tag/opencv">opencv</a></li>
		
		<li><a href="/tag/bovw">bovw</a></li>
		
	</ul>
</div>

<div class="post-content">
	<p>
	<p><img src="/images/fleuron/headpiece.png" alt="headpiece"></p>

<h2 id="the-fleuron-project">The Fleuron Project</h2>

<p>I have been involved in the Fleuron project this year. The aim of this project is to use computer vision to extract printers&#39; ornaments from a large corpus of ~150,000 scanned documents (32 million pages) from the 18th century. Printed books in the 18th century were highly decorated with miniature pieces of printed artwork - &#39;ornaments&#39;. Their pages were adorned with ornaments that ranged from small floral embellishments to large and intricate head- and tailpieces, depicting all manner of people, places, and things. Printers&#39; ornaments are of interest to historians from many disciplines, not least for their importance as examples of early graphic design and craftsmanship. They can help solve the mysteries of the book trade, and they can be used to detect piracy and fraud.</p>

<p>In this project an OpenCV based code was developed to automatically detect ornaments in a scanned image of a page and extract them into their own file. This code worked very well and extracted over 3 million images from the data, but it was quite over-sensitive in its detection so there were many false-positives. The code was heuristic based and didn&#39;t use any machine intelligence to further evaluate the potential images for validity. We therefore chose to tune the code to have good recall at the expense of precision -- i.e. we would rather it didn&#39;t miss valid images, even if it means that some invalid images get through too. Often these invalid images were of blocks of text so we initially experimented with using OCR to catch these cases. However this had the unwanted effect of making recall worse. We decided a better solution would be to train a machine learning classifier to discriminate between the valid and invalid images. </p>

<p>My contribution to the project was to use the extraction code to generate data, which I then hand-labelled to create a training set to train a machine learning based filter to remove the bad images. The final filtered dataset is presented on the website: <a href="http://fleuron.lib.cam.ac.uk">http://fleuron.lib.cam.ac.uk</a>, which I also designed and built.  In this blog post I will describe the methodology and results of the image filtering part of the project. </p>

<h2 id="extraction">Extraction</h2>

<p>The first challenge is to extract the ornaments from the raw page scans. This is an example of a page containing two ornaments, at the top of the page and at the start of the text body:
<img src="/images/fleuron/example_page.png" alt="Typical Page"></p>

<p>We required an algorithm that could ignore the text and draw bounding boxes around the two ornaments on the page. To solve this problem we enlisted <a href="https://dirkgorissen.com/">Dirk Gorissen</a> to develop a method using Python and OpenCV. I will not dive deeply into how Dirk&#39;s algorithm works here. Basically it uses combines heuristics of where ornaments are typically located and how they look with various image filtering techniques to weed out text and other artifacts on the page to leave just the artwork intact.</p>

<p>Here is a demonstration of how each of the different stages of the algorithm work using on the single page shown above as an example:</p>

<p><img alt="Extracting ornaments" src="/images/fleuron/debug_1.png" width="500"></p>

<p>Ornaments are visually very dense compared to the text. In the first stage the image is cleaned removing dust and stains in the white space of the page. Then through several iterations of blurring and contouring are applied until just the ornaments are left as single contours as seen in stage 5. A bounding box is then drawn around these contours and content of these boxes is then extracted from the original image. </p>

<p>This method is simple and effective, but it is also apt to falsely classifying blocks of text. In the following example you can see clearly how this can happen:</p>

<p><img alt="Extracting ornaments with a false positive" src="/images/fleuron/debug_2.png" width="500"></p>

<p>After running extraction on all of the pages, I found that in a random sample of the images, most of them were just images of blocks of text! However, given that most of the pages in the dataset contain only text and no ornaments, perhaps this is to be expected even if the algorithm is fairly good at removing text. </p>

<p>After extracting the ornaments from a large sample of the books, I hand labeled a random sample of 15000 images as valid and invalid. Here is a collage of valid images:</p>

<p><img alt="Examples of valid images" src="/images/fleuron/collage_valid.jpg" width="500"></p>

<p><br>
Here is a collage of invalid images that we want to filter out:</p>

<p><img alt="Examples of invalid images" src="/images/fleuron/collage_invalid.jpg" width="500"></p>

<h2 id="image-filtering-pipeline">Image Filtering Pipeline</h2>

<p>The choice of image representation is essential to getting a well performing machine learning based classifier. The Images are black and white and so we can&#39;t use any colour features and contain very rich textures. 
The pipeline for the image filtering system: </p>

<ol>
<li>Create a labelled data set for training</li>
<li>Represent each training image by a vector using Bag of Visual Words</li>
<li>Train a classifier on the vector to discriminate between valid and invalid images</li>
<li>Apply the classifier to unseen images in the data set.</li>
</ol>

<h2 id="bag-of-visual-words-bovw">Bag of Visual Words (BoVW)</h2>

<p>The Bag of Visual Words (BoVW) method is a common feature representation of images in computer vision. The method is directly inspired by the Bag of Words (BoW) method used in <em>text classificiation</em>. In the BoW method, the basic idea is that a text document is split up into its component words. Each of the words in the document is then matched to a word in the dictionary, and the number of unique words in the document is counted. The text document is then represented as a sparse <em>histogram</em> of word counts that is as long as the dictionary. </p>

<p>This histogram can be interpreted as a vector in some high dimensional space, and two different documents will be represented by two different vectors. So for a dictionary with $D$ words the vector for document $i$ is:</p>

<p>$$
v_i = [n(w_1,i), n(w_2,i), ..., n(w_D, i)] 
$$
Where $n(w)$ counts the number of occurrences of word $w$. The distance between these two vectors (e.g. L2, cosine, etc) can therefore be used as a proxy for the <strong>similarity</strong> of the two documents. If everything is working well, then a low distance will indicate high similarity and a large distance will represent a high dissimilarity. With this representation we are able to throw machine learning algorithms at the data or do document retrieval. </p>

<p>BoVW is exactly the same method except that instead of using actual words it uses &#39;visual words&#39; extracted from the images. Visual words basically take the form of &#39;iconic&#39; patches or fragments of an image. </p>

<h3 id="1-extract-notable-features-from-the-images">1. Extract notable features from the images</h3>

<p><img src="/images/fleuron/image_to_words_1.jpg" width=500></p>

<p><img src="/images/fleuron/image_to_words_2.jpg" width=500></p>

<h3 id="2-learn-a-visual-dictionary">2. Learn a visual dictionary</h3>

<p>Use a clustering algorithm like k-means with a apriori number of clusters (&gt;1000) to learn a set of $k$ compound visual words. </p>

<p><img src="/images/fleuron/image_dict.png" width=500></p>

<h3 id="3-quantize-features-using-the-visual-vocabulary">3. Quantize features using the visual vocabulary</h3>

<p>Now we could then take an image, find its visual words and match each of those words to their nearest equivalent in the dictionary.</p>

<h3 id="4-represent-images-by-histogram-of-visual-word-counts">4. Represent images by histogram of visual word counts</h3>

<p>By counting how many times a word in the dictionary is matched, the image can be re-represented as a histogram of word counts:</p>

<p><img src="/images/fleuron/image_hist.png" alt="Historgram of visual word counts"></p>

<p>Similar looking images will have contains many of the same words and counts.</p>

<h2 id="sift-the-visual-word">SIFT - The Visual Word</h2>

<p>Now that we have outlined the concept of the BoVW method, what do we actually use as the &#39;visual word&#39;? To create the visual words I used SIFT - &#39;Scale Invariant Feature Transform&#39;. SIFT is a method for detecting multiple interesting <em>keypoints</em> in a grey-scale image and describing each of those points using a 128 dimensional vector. The SIFT descriptor is invariant to scale, rotation, and illumination, which is why it is such a popular method in classification and CBIR. An excellent technical description of SIFT can be found <a href="http://www.scholarpedia.org/article/Scale_Invariant_Feature_Transform">here</a>. </p>

<p>OpenCV has an implementation of a SIFT detector included. The following code finds all the keypoints in an image and draws them back onto the image.</p>
<div class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">img</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">imread</span><span class="p">(</span><span class="s">&#39;image_2.png&#39;</span><span class="p">)</span>
<span class="n">img_gray</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">cvtColor</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="n">cv2</span><span class="o">.</span><span class="n">COLOR_BGR2GRAY</span><span class="p">)</span>
<span class="n">kp</span><span class="p">,</span> <span class="n">desc</span> <span class="o">=</span> <span class="n">sift</span><span class="o">.</span><span class="n">detectAndCompute</span><span class="p">(</span><span class="n">img_gray</span><span class="p">,</span> <span class="bp">None</span><span class="p">)</span>
<span class="n">img2</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">drawKeypoints</span><span class="p">(</span><span class="n">img_gray</span><span class="p">,</span> <span class="n">kp</span><span class="p">,</span> <span class="n">flags</span><span class="o">=</span><span class="n">cv2</span><span class="o">.</span><span class="n">DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS</span><span class="p">)</span>

<span class="n">cv2</span><span class="o">.</span><span class="n">imwrite</span><span class="p">(</span><span class="s">&#39;image_sift_2.png&#39;</span><span class="p">,</span> <span class="n">img2</span><span class="p">)</span>
</code></pre></div>
<p>Here is the output of this for two images from the dataset, one valid and the other invalid:
<img src="/images/fleuron/image_sift_1.png" alt="SIFT keypoints"></p>

<p><img src="/images/fleuron/image_sift_2.png" alt="SIFT keypoints"></p>

<p>There is a simple improvement that can be made to SIFT called <a href="https://www.robots.ox.ac.uk/%7Evgg/publications/2012/Arandjelovic12/presentation.pdf">RootSIFT</a>.  RootSIFT is a small modification to the SIFT descriptor that corrects the L2 distance between two SIFT descriptors. This generally always improves performance for classification and image retrieval. Here is an implementation in python:</p>
<div class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">rootsift_descriptor</span><span class="p">(</span><span class="n">f</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Extract root sift descriptors from image stored in file f</span>
<span class="sd">    :param: f : str or unicode filename</span>
<span class="sd">    :return: desc : numpy array [n_sift_desc, 128]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">img</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">imread</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>
    <span class="n">img_gray</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">cvtColor</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="n">cv2</span><span class="o">.</span><span class="n">COLOR_BGR2GRAY</span><span class="p">)</span>
    <span class="n">sift</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">SIFT</span><span class="p">()</span>
    <span class="n">kp</span><span class="p">,</span> <span class="n">desc</span> <span class="o">=</span> <span class="n">sift</span><span class="o">.</span><span class="n">detectAndCompute</span><span class="p">(</span><span class="n">img_gray</span><span class="p">,</span> <span class="bp">None</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">desc</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
        <span class="k">print</span><span class="p">(</span><span class="s">&#39;Warning: No SIFT features found in {}&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">f</span><span class="p">))</span>
        <span class="k">return</span> <span class="bp">None</span>

    <span class="n">desc</span> <span class="o">/=</span> <span class="p">(</span><span class="n">desc</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span> <span class="o">+</span> <span class="mf">1e-7</span><span class="p">)</span>
    <span class="n">desc</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">desc</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">desc</span>
</code></pre></div>
<h2 id="building-a-visual-word-dictionary">Building a Visual Word Dictionary</h2>

<p>To create a visual word dictionary we need to first collect and store all the RootSIFT descriptors from a large sample of images from our dataset. Here I used a sample size of 50,000 images. For 50,000 images, $N \approx 1~ billion$.  The following script iterates through every image in the target directory, finds the RootSIFT descriptors of the image, and then stores them in a large $N\times128$ array in a HDF5 file. </p>
<div class="highlight"><pre><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">glob</span>
<span class="kn">import</span> <span class="nn">cv2</span>
<span class="kn">import</span> <span class="nn">tables</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">bovw</span> <span class="kn">import</span> <span class="n">rootsift_descriptor</span>
<span class="kn">from</span> <span class="nn">random</span> <span class="kn">import</span> <span class="n">sample</span><span class="p">,</span> <span class="n">shuffle</span>


<span class="k">def</span> <span class="nf">create_descriptor_bag</span><span class="p">(</span><span class="n">filelist</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    creates an array of descriptor vectors generated from every</span>
<span class="sd">    file in filelist</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">empty</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="mi">128</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="n">N</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">filelist</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">n</span><span class="p">,</span> <span class="n">f</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">filelist</span><span class="p">):</span>
        <span class="k">print</span><span class="p">(</span><span class="s">&#39;Processing file: {} of {}...&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">N</span><span class="p">))</span>
        <span class="n">desc</span> <span class="o">=</span> <span class="n">rootsift_descriptor</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">desc</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
            <span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">((</span><span class="n">X</span><span class="p">,</span> <span class="n">desc</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">X</span>


<span class="k">def</span> <span class="nf">main</span><span class="p">():</span>
    <span class="n">h5f</span> <span class="o">=</span> <span class="n">tables</span><span class="o">.</span><span class="n">openFile</span><span class="p">(</span><span class="s">&#39;/scratch/ECCO/rootsift_vectors_50k.hdf&#39;</span><span class="p">,</span> <span class="s">&#39;w&#39;</span><span class="p">)</span>
    <span class="n">atom</span> <span class="o">=</span> <span class="n">tables</span><span class="o">.</span><span class="n">Atom</span><span class="o">.</span><span class="n">from_dtype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">dtype</span><span class="p">(</span><span class="s">&#39;Float32&#39;</span><span class="p">))</span>
    <span class="n">ds</span> <span class="o">=</span> <span class="n">h5f</span><span class="o">.</span><span class="n">createEArray</span><span class="p">(</span><span class="n">h5f</span><span class="o">.</span><span class="n">root</span><span class="p">,</span> <span class="s">&#39;descriptors&#39;</span><span class="p">,</span> <span class="n">atom</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">128</span><span class="p">),</span> <span class="n">expectedrows</span><span class="o">=</span><span class="mi">1000000</span><span class="p">)</span>

    <span class="n">PATH</span> <span class="o">=</span> <span class="s">&#39;/home/jb914/ECCO_dict/random50k/&#39;</span>
    <span class="n">all_files</span> <span class="o">=</span> <span class="n">glob</span><span class="o">.</span><span class="n">glob</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">PATH</span><span class="p">,</span> <span class="s">&#39;*.png&#39;</span><span class="p">))</span>
    <span class="n">rand_sample</span> <span class="o">=</span> <span class="p">[</span><span class="n">all_files</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">sample</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">all_files</span><span class="p">)),</span> <span class="mi">5000</span><span class="p">)]</span>
    <span class="n">chunk_size</span> <span class="o">=</span> <span class="mi">100</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">xrange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="nb">len</span><span class="p">(</span><span class="n">rand_sample</span><span class="p">),</span> <span class="n">chunk_size</span><span class="p">):</span>
        <span class="k">print</span><span class="p">(</span><span class="s">&#39;Creating rootsift descriptor bag...&#39;</span><span class="p">)</span>
        <span class="n">X</span> <span class="o">=</span> <span class="n">create_descriptor_bag</span><span class="p">(</span><span class="n">rand_sample</span><span class="p">[</span><span class="n">i</span><span class="p">:</span><span class="n">i</span><span class="o">+</span><span class="n">chunk_size</span><span class="p">])</span>
        <span class="k">print</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
        <span class="k">print</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="p">))</span>

        <span class="k">print</span><span class="p">(</span><span class="s">&#39;Writing file: rootsift_vectors_5000.hdf&#39;</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">xrange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="p">)):</span>
            <span class="n">ds</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="bp">None</span><span class="p">])</span>
        <span class="n">ds</span><span class="o">.</span><span class="n">flush</span><span class="p">()</span>
    <span class="c">#ds[:] = X</span>
    <span class="n">h5f</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>


<span class="k">if</span> <span class="n">__name__</span> <span class="o">==</span> <span class="s">&#39;__main__&#39;</span><span class="p">:</span>
    <span class="n">main</span><span class="p">()</span>
</code></pre></div>
<p>HDF5 files are great for storing large multidimensional arrays of data to disk because they store the meta-data of the array dimensions and allow for streaming the data from disk to memory. This is especially useful when the full data is much larger than memory like here. </p>

<p>Creating the dictionary is the hardest and most time consuming part with BoVW. There are many vectors to cluster, the number of words is very large (between 1000 and 1,000,000), and the vectors are high dimensional. This stretches the capabilities of many clustering algorithms in all possible ways. In my experiments I found that the standard K-Means clustering algorithm quickly became intractable for larger numbers of vectors and clusters. Moreover the algorithm is offline - it needs to see all the data at once. Algorithms better suited for this task are approximate k-means (AKM) and mini-batch k-means. </p>

<p>I found success with two open source implementations of these in <a href="https://github.com/philbinj/fastcluster">fastcluster</a> (AKM), and in <a href="http://scikit-learn.org/stable/modules/generated/sklearn.cluster.MiniBatchKMeans.html">MiniBatchKMeans</a> from scikit-learn. Fastcluster has the advantage that it uses distributed parallelism via MPI to split the large data up across multiple machines. However this useful code lacks documentation and no longer maintained. MiniBatchKMeans on the other hand isn&#39;t parallel, however it does allow for streaming of the data through memory so it works great with HDF5. </p>

<p>In my experiments I found that setting the dictionary size to 20,000 words was sufficient.</p>

<p>The following script can stream a HDF5 file in a user defined number of chunks performing clustering with those chunks. The total clustering time for this was approximately 24 hours running in serial on a Intel Xeon Ivybridge CPU. </p>
<div class="highlight"><pre><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">__future__</span> <span class="kn">import</span> <span class="n">print_function</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">tables</span>
<span class="kn">import</span> <span class="nn">pickle</span>
<span class="kn">from</span> <span class="nn">sklearn.cluster</span> <span class="kn">import</span> <span class="n">MiniBatchKMeans</span>
<span class="kn">from</span> <span class="nn">time</span> <span class="kn">import</span> <span class="n">time</span>


<span class="k">def</span> <span class="nf">main</span><span class="p">(</span><span class="n">n_clusters</span><span class="p">,</span> <span class="n">chunk</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">n_chunks</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="n">checkpoint_file</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
    <span class="n">datah5f</span> <span class="o">=</span> <span class="n">tables</span><span class="o">.</span><span class="n">open_file</span><span class="p">(</span><span class="s">&#39;/scratch/ECCO/rootsift_vectors_50k.hdf&#39;</span><span class="p">,</span> <span class="s">&#39;r&#39;</span><span class="p">)</span>
    <span class="n">shape</span> <span class="o">=</span> <span class="n">datah5f</span><span class="o">.</span><span class="n">root</span><span class="o">.</span><span class="n">descriptors</span><span class="o">.</span><span class="n">shape</span>
    <span class="n">datah5f</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>

    <span class="k">print</span><span class="p">(</span><span class="s">&#39;Read in SIFT data with size in chunks: &#39;</span><span class="p">,</span> <span class="n">shape</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="s">&#39;Running MiniBatchKMeans with cluster sizes: &#39;</span><span class="p">,</span> <span class="n">n_clusters</span><span class="p">)</span>

    <span class="n">c</span> <span class="o">=</span> <span class="n">n_clusters</span>
    <span class="k">print</span><span class="p">(</span><span class="s">&#39;n_clusters:&#39;</span><span class="p">,</span> <span class="n">c</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">checkpoint_file</span><span class="p">:</span>
        <span class="n">mbkm</span> <span class="o">=</span> <span class="n">pickle</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="nb">open</span><span class="p">(</span><span class="n">checkpoint_file</span><span class="p">,</span> <span class="s">&#39;r&#39;</span><span class="p">))</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">mbkm</span> <span class="o">=</span> <span class="n">MiniBatchKMeans</span><span class="p">(</span><span class="n">n_clusters</span><span class="o">=</span><span class="n">c</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">10000</span><span class="p">,</span> <span class="n">init_size</span><span class="o">=</span><span class="mi">30000</span><span class="p">,</span> <span class="n">init</span><span class="o">=</span><span class="s">&#39;random&#39;</span><span class="p">,</span> <span class="n">compute_labels</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>

    <span class="n">step</span> <span class="o">=</span> <span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">/</span> <span class="n">n_chunks</span>
    <span class="n">start_i</span> <span class="o">=</span> <span class="n">chunk</span><span class="o">*</span><span class="n">step</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">xrange</span><span class="p">(</span><span class="n">start_i</span><span class="p">,</span> <span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">step</span><span class="p">):</span>
        <span class="n">datah5f</span> <span class="o">=</span> <span class="n">tables</span><span class="o">.</span><span class="n">open_file</span><span class="p">(</span><span class="s">&#39;/scratch/ECCO/rootsift_vectors_{}.hdf&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">datasize</span><span class="p">),</span> <span class="s">&#39;r&#39;</span><span class="p">)</span>
        <span class="n">X</span> <span class="o">=</span> <span class="n">datah5f</span><span class="o">.</span><span class="n">root</span><span class="o">.</span><span class="n">descriptors</span><span class="p">[</span><span class="n">i</span><span class="p">:</span><span class="n">i</span><span class="o">+</span><span class="n">step</span><span class="p">]</span>
        <span class="n">datah5f</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>

        <span class="n">t0</span> <span class="o">=</span> <span class="n">time</span><span class="p">()</span>
        <span class="n">mbkm</span><span class="o">.</span><span class="n">partial_fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="k">print</span><span class="p">(</span><span class="s">&#39;</span><span class="se">\t</span><span class="s"> ({} of {}) Time taken: {}&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">chunk</span><span class="p">,</span> <span class="n">n_chunks</span><span class="p">,</span> <span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">t0</span><span class="p">))</span>
        <span class="n">chunk</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="n">pickle</span><span class="o">.</span><span class="n">dump</span><span class="p">(</span><span class="n">mbkm</span><span class="p">,</span> <span class="nb">open</span><span class="p">(</span><span class="s">&#39;chkpt_{}.p&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">n_clusters</span><span class="p">),</span> <span class="s">&#39;w&#39;</span><span class="p">))</span>

    <span class="n">X</span> <span class="o">=</span> <span class="n">mbkm</span><span class="o">.</span><span class="n">cluster_centers_</span>
    <span class="k">print</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="n">f</span> <span class="o">=</span> <span class="n">tables</span><span class="o">.</span><span class="n">open_file</span><span class="p">(</span><span class="s">&#39;fleuron_codebook_{}.hdf&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">n_clusters</span><span class="p">),</span> <span class="s">&#39;w&#39;</span><span class="p">)</span>
    <span class="n">atom</span> <span class="o">=</span> <span class="n">tables</span><span class="o">.</span><span class="n">Atom</span><span class="o">.</span><span class="n">from_dtype</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="n">ds</span> <span class="o">=</span> <span class="n">f</span><span class="o">.</span><span class="n">create_carray</span><span class="p">(</span><span class="n">f</span><span class="o">.</span><span class="n">root</span><span class="p">,</span> <span class="s">&#39;clusters&#39;</span><span class="p">,</span> <span class="n">atom</span><span class="p">,</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="n">ds</span><span class="p">[:]</span> <span class="o">=</span> <span class="n">X</span>
    <span class="n">f</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>


<span class="k">if</span> <span class="n">__name__</span> <span class="o">==</span> <span class="s">&#39;__main__&#39;</span><span class="p">:</span>
    <span class="n">main</span><span class="p">(</span><span class="n">n_clusters</span><span class="o">=</span><span class="mi">20000</span><span class="p">,</span> <span class="n">chunk</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">n_chunks</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">checkpoint_file</span><span class="o">=</span><span class="bp">None</span><span class="p">)</span>
</code></pre></div>
<h2 id="matching-key-points-to-the-dictionary">Matching Key Points to the Dictionary</h2>

<p>With the dictionary created the next step is to represent all the images in the labeled training set as a histogram of matching keypoints. This is a nearest neighbour matching problem so with a brute force algorithm this is a $O(N)$ so this is slow for very high numbers of words. Faster nearest neighbour matching can be achieved with the <a href="http://www.cs.ubc.ca/research/flann/">FLANN</a> library. OpenCV contains a wrapper for FLANN. I wrote a class that uses the FLANN matcher in OpenCV to match an array of descriptors to a codebook:</p>
<div class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">FLANN_INDEX_COMPOSITE</span> <span class="o">=</span> <span class="mi">3</span>
<span class="n">FLANN_DIST_L2</span> <span class="o">=</span> <span class="mi">1</span>


<span class="k">class</span> <span class="nc">Codebook</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hdffile</span><span class="p">):</span>
        <span class="n">clusterf</span> <span class="o">=</span> <span class="n">tables</span><span class="o">.</span><span class="n">open_file</span><span class="p">(</span><span class="n">hdffile</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_clusters</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">clusterf</span><span class="o">.</span><span class="n">get_node</span><span class="p">(</span><span class="s">&#39;/clusters&#39;</span><span class="p">))</span>
        <span class="n">clusterf</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_clusterids</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="nb">xrange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_clusters</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">int</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_clusters</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_clusters</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_flann</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">flann_Index</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_clusters</span><span class="p">,</span>
                                      <span class="nb">dict</span><span class="p">(</span><span class="n">algorithm</span><span class="o">=</span><span class="n">FLANN_INDEX_COMPOSITE</span><span class="p">,</span>
                                           <span class="n">distance</span><span class="o">=</span><span class="n">FLANN_DIST_L2</span><span class="p">,</span>
                                           <span class="n">iterations</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
                                           <span class="n">branching</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span>
                                           <span class="n">trees</span><span class="o">=</span><span class="mi">50</span><span class="p">))</span>

    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">Xdesc</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Takes Xdesc a (n,m) numpy array of n img descriptors length m and returns</span>
<span class="sd">        (n,1) where every n has been assigned to a cluster id.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="p">(</span><span class="n">_</span><span class="p">,</span> <span class="n">m</span><span class="p">)</span> <span class="o">=</span> <span class="n">Xdesc</span><span class="o">.</span><span class="n">shape</span>
        <span class="p">(</span><span class="n">_</span><span class="p">,</span> <span class="n">cm</span><span class="p">)</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_clusters</span><span class="o">.</span><span class="n">shape</span>
        <span class="k">assert</span> <span class="n">m</span> <span class="o">==</span> <span class="n">cm</span>

        <span class="n">result</span><span class="p">,</span> <span class="n">dists</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_flann</span><span class="o">.</span><span class="n">knnSearch</span><span class="p">(</span><span class="n">Xdesc</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">params</span><span class="o">=</span><span class="p">{})</span>
        <span class="k">return</span> <span class="n">result</span>
</code></pre></div>
<p>The following code takes a list of image files and a dictionary and returns the count vectors for each of those image files using the <code>sparse</code> matrix type from <code>scipy</code>. It is also multi-threaded using the <code>joblib</code> library:</p>
<div class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">count_vector</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">codebook</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Takes a list of SIFT vectors from an image and matches</span>
<span class="sd">    each SIFT vector to its nearest equivalent in the codebook</span>
<span class="sd">    :param: f : Image file path</span>
<span class="sd">    :return: countvec : sparse vector of counts for each visual-word in the codebook</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">desc</span> <span class="o">=</span> <span class="n">rootsift_descriptor</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">desc</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
        <span class="c"># if no sift features found return 0 count vector</span>
        <span class="k">return</span> <span class="n">lil_matrix</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="n">codebook</span><span class="o">.</span><span class="n">n_clusters</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">int</span><span class="p">)</span>

    <span class="n">matches</span> <span class="o">=</span> <span class="n">codebook</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">desc</span><span class="p">)</span>
    <span class="n">unique</span><span class="p">,</span> <span class="n">counts</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">matches</span><span class="p">,</span> <span class="n">return_counts</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

    <span class="n">countvec</span> <span class="o">=</span> <span class="n">lil_matrix</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="n">codebook</span><span class="o">.</span><span class="n">n_clusters</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">int</span><span class="p">)</span>
    <span class="n">countvec</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="n">unique</span><span class="p">]</span> <span class="o">=</span> <span class="n">counts</span>
    <span class="k">return</span> <span class="n">countvec</span>


<span class="k">class</span> <span class="nc">CountVectorizer</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">vocabulary_file</span><span class="p">,</span> <span class="n">n_jobs</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_codebook</span> <span class="o">=</span> <span class="n">Codebook</span><span class="p">(</span><span class="n">hdffile</span><span class="o">=</span><span class="n">vocabulary_file</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">n_jobs</span> <span class="o">==</span> <span class="o">-</span><span class="mi">1</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">n_jobs</span> <span class="o">=</span> <span class="n">cpu_count</span><span class="p">()</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">n_jobs</span> <span class="o">=</span> <span class="n">n_jobs</span>

    <span class="k">def</span> <span class="nf">transform</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">images</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Transform image files to a visual-word count matrix.</span>
<span class="sd">        :param: images : iterable</span>
<span class="sd">                    An iterable of str or unicode filenames</span>
<span class="sd">        :return: X : sparse matrix, [n_images, n_visual_words]</span>
<span class="sd">                     visual-word count matrix</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="n">sparse_rows</span> <span class="o">=</span> <span class="n">Parallel</span><span class="p">(</span><span class="n">backend</span><span class="o">=</span><span class="s">&#39;threading&#39;</span><span class="p">,</span> <span class="n">n_jobs</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">n_jobs</span><span class="p">)(</span>
            <span class="p">(</span><span class="n">delayed</span><span class="p">(</span><span class="n">count_vector</span><span class="p">)(</span><span class="n">f</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_codebook</span><span class="p">)</span> <span class="k">for</span> <span class="n">f</span> <span class="ow">in</span> <span class="n">images</span><span class="p">)</span>
        <span class="p">)</span>

        <span class="n">X</span> <span class="o">=</span> <span class="n">lil_matrix</span><span class="p">((</span><span class="nb">len</span><span class="p">(</span><span class="n">images</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">_codebook</span><span class="o">.</span><span class="n">n_clusters</span><span class="p">),</span>
                       <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">int</span><span class="p">)</span>

        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">sparse_row</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">sparse_rows</span><span class="p">):</span>
            <span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">sparse_row</span>

        <span class="k">return</span> <span class="n">X</span><span class="o">.</span><span class="n">tocsr</span><span class="p">()</span>
</code></pre></div>
<p>Given a list of files the following code will return the count vectors for those images:</p>
<div class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">vectorizer</span> <span class="o">=</span> <span class="n">CountVectorizer</span><span class="p">(</span><span class="s">&#39;codebook_20k.hdf&#39;</span><span class="p">,</span> <span class="n">n_jobs</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<span class="n">Xcounts</span> <span class="o">=</span> <span class="n">vectorizer</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">images</span><span class="p">)</span>
</code></pre></div>
<h3 id="tf-idf">tf-idf</h3>

<p>Not all words are created equal, some are more frequent than others. This is the same in human language and in the create visual vocabulary. Words like &#39;the&#39;, &#39;what&#39;, &#39;where&#39;, etc will swamp the count vectors of english words in almost all english documents. Clearly they are less interesting than a rare word like &#39;disestablishment&#39; and that we&#39;d like two different documents both containing a word like &#39;disestablishment&#39; to have a high similarity. So we&#39;d like to reweight words which appear in few documents so that they have a higher importance, and words that appear in most documents to have lower importance. 
In another case, if a document only contained the word &#39;disestablishment&#39; 1000 times should it be 1000 times more relevant than a document containing it once? So within an individual document we may want to reweight words that are repeated over and over so that they cannot artificially dominate.</p>

<p>These two reweightings can be achieved using <a href="https://en.wikipedia.org/wiki/Tf%E2%80%93idf"><em>tf-idf</em></a> (term frequency inverse document frequency) weighting. This weighting is designed to reflect how important a particular word is in a document corpus. It is perfectly applicable in our visual word case also. <a href="http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html#sklearn.feature_extraction.text.TfidfTransformer">Scikit-learn</a> has an implementation of a tf-idf transformer for text classification that we can repurpose here. To following produces the final representation for the training data that we can use in the machine learning algorithm, $X$.</p>
<div class="highlight"><pre><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">sklearn.feature_extraction.text</span> <span class="kn">import</span> <span class="n">TfidfTransformer</span>
<span class="n">transformer</span> <span class="o">=</span> <span class="n">TfidfTransformer</span><span class="p">()</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">transformer</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">Xcount</span><span class="p">)</span>
</code></pre></div>
<h2 id="visualisation-of-bovw">Visualisation of BoVW</h2>

<p>That that we&#39;ve transformed the images into tf-idf weighted, 20k dimensional, sparse vectors to visual word counts, we can visualise them and see if there is any apparent structure in this high dimensional data. Great algorithms for visualising high dimensional data are PCA and T-SNE, both of which have implementations in scikit-learn. I found here that PCA worked best. For high dimensional sparse data, the <code>TruncatedSVD</code> algorithm works best:</p>
<div class="highlight"><pre><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">sklearn.decomposition</span> <span class="kn">import</span> <span class="n">TruncatedSVD</span>

<span class="n">svd</span> <span class="o">=</span> <span class="n">TruncatedSVD</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">Z</span> <span class="o">=</span> <span class="n">svd</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
</code></pre></div>
<p>We can plot this with the images inlined and with colours representing the valid (red) and invalid (blue) labels:</p>

<p><img src="/images/fleuron/pca_both.png" alt="PCA viz"></p>

<p>You can clearly see that there is structure in the higher dimensions and that the valid and invalid images separate quite well from each other. This is quite promising for the performance of a machine learning algorithm!</p>

<h2 id="classifying-the-bad-images-with-machine-learning">Classifying the Bad Images with Machine Learning</h2>

<p>I tried a number of algorithms including Random forest, logistic regression and linear SVM. I found that SVM with a linear kernel by far performed the best compared to the other algorithms.</p>
<div class="highlight"><pre><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">sklearn.svm</span> <span class="kn">import</span> <span class="n">LinearSVC</span>

<span class="n">clf</span> <span class="o">=</span> <span class="n">LinearSVC</span><span class="p">()</span>
<span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</code></pre></div>
<p><code>LinearSVC</code> with the default settings performed very well with $97\%$ accuracy. High accuracy is generally a good sign, especially here where the numbers of valid and invalid images are of a similar size. Two other important statistics for classification are <strong>precision</strong> and <strong>recall</strong>.</p>

<p>Recall is a measure of what the probability that the classifier will identify an image as invalid given that it is invalid: $P(\hat{y}=1 | y=1)$. You can think of recall as the ratio of the <strong>number of images correctly classed as invalid</strong> to the number of <strong>all invalid images</strong>. </p>

<p>Precision on the other hand is a measure of the probability that an image is invalid given that the classifier says it is invalid: $P(y=1|\hat{y}=1)$.  You can think of precision as the ratio of the <strong>number of images correctly classed as invalid</strong> to the number of <strong>all images classified</strong>.</p>

<p>The difference between them is subtle (<a href="https://www.quora.com/What-is-the-best-way-to-understand-the-terms-precision-and-recall">here</a> is a great explanation of the difference), but you may want to favour a trade-off of one for the other depending on your <em>business case</em>. In our case it is <em>worse</em> to misclassify a valid images as invalid because we are losing good images. We would much rather have some invalid images get through than lose good images, which is the same as favouring extra precision over recall.</p>

<p>We can tune the precision by adjusting the <strong>class weights</strong> of the Linear SVM, such that the penalty for classifying a valid image as invalid is <em>much worse</em> than classifying an invalid image as valid. I used cross-validation to find the best values for these. These give the valid images a weight of 20 and invalid images a weight of 0.1:</p>
<div class="highlight"><pre><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">sklearn.svm</span> <span class="kn">import</span> <span class="n">LinearSVC</span>

<span class="n">clf</span> <span class="o">=</span> <span class="n">LinearSVC</span><span class="p">(</span><span class="n">class_weight</span><span class="o">=</span><span class="p">{</span><span class="mi">0</span><span class="p">:</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">1</span><span class="p">:</span> <span class="mf">0.1</span><span class="p">})</span>
<span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</code></pre></div>
<p>This yielded the final performance of $95\%$ accuracy, $99.5\%$ precision, and $93.8\%$ recall:</p>
<div class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">Confusion</span> <span class="n">matrix</span><span class="p">:</span> 
 <span class="p">[[</span><span class="mi">1134</span>   <span class="mi">11</span><span class="p">]</span>
 <span class="p">[</span> <span class="mi">161</span> <span class="mi">2439</span><span class="p">]]</span>
<span class="n">Accuracy</span> <span class="n">score</span><span class="p">:</span>  <span class="mf">0.954072096128</span>
<span class="bp">False</span> <span class="n">Positive</span> <span class="n">Rate</span><span class="p">:</span>  <span class="mf">0.00960698689956</span>
<span class="bp">False</span> <span class="n">Negative</span> <span class="n">Rate</span><span class="p">:</span>  <span class="mf">0.0619230769231</span>
<span class="n">Precision</span><span class="p">:</span>  <span class="mf">0.995510204082</span>
<span class="n">Recall</span><span class="p">:</span>  <span class="mf">0.938076923077</span>
<span class="n">F1</span> <span class="n">Score</span><span class="p">:</span>  <span class="mf">0.965940594059</span>
</code></pre></div>
<p>Overall the performance of this approach is very good. In the end after applying the trained classifier to the whole image dataset, there were approximately 3 million images classed as invalid and 2 million images classed as valid.
You can inspect some of images in the filtered dataset online <a href="https://fleuron.lib.cam.ac.uk/ornament_search_results?h2=400&amp;h1=200&amp;w2=1000&amp;w1=500&amp;page=1">here</a>. There are very few bad images that have managed to make it through. The ones that have managed to make it into the dataset often have the same characteristics. </p>

<p>For example, they are commonly images of <strong>handwriting</strong>:</p>

<p><img alt="handwriting" src="/images/fleuron/false_neg_1.png" style="width: 400" /> </p>

<p>Compared to printed text, handwriting is appears very flourished and irregular, much like the ornaments. These images are also quite rare so not many appeared in the training set allowing the classifier to discriminate them well.</p>

<p>Another common one is <strong>library stamps</strong> which are not present in the original text, but have been stamped on by the libraries that have kept the books over the years:</p>

<p><img alt="stamps" src="/images/fleuron/false_neg_2.png" style="width: 400" /> 
<img alt="stamps" src="/images/fleuron/false_neg_3.png" style="width: 400" /> </p>

<p>These were quite common in the training data, but they look so much like ornaments themselves that the classifier couldn&#39;t quite learn to separate them.</p>

<h2 id="further-ideas">Further Ideas</h2>

<h3 id="image-search">Image search</h3>

<p>The BoVW approach is also very useful for image retrieval. This means that given some image we can find duplications and similar looking images in the rest of the image set simply by finding the BoVW vectors that are closest to that image&#39;s own BoVW vector. This is just a nearest neighbour search. It is complicated by the number of images because scaling nearest neighbour search with large numbers of vectors that don&#39;t necessarily fit into memory relies on more complicated algorithms. </p>

<h3 id="neural-networks">Neural Networks</h3>

<p>Convolutional Neural Networks (CNNs) have shown great application in image classification in recent years. While they perform well at classification, they also have the advantage that they can discover vector representations of the images given the just the raw pixels. So it doesn&#39;t require all this work with inventing a representation for images such as BoVW. The downside is that they require a lot of data (10s of thousands of examples) to be effective. Rather than hand labelling more examples, it would be quicker to look at the output of images classified by the SVM, and eyeball any false negatives or false positives in there. Artificial data could also be created using image transformations like rotation and inversion. </p>

	</p>
</div>

<div class="post-footer">
  <div class="column-full">
    <h3><a href="/archive.html">Blog archive</a></h3>
  </div>
</div>



          </div>
        </div>
        <!-- disable side-bar
        <div class="col-md-3 hidden-xs">
          <div class="sidebar ">
  <h1>Recent Posts</h1>
  <ul>
    
    <li><a href="/2016/12/fleuronbovw">SIFTing Images</a></li>
    
    <li><a href="/2016/11/polyclojure3">Polymorphism in Clojure: A Tutorial Using Numbers, Part 3</a></li>
    
    <li><a href="/2016/11/polyclojure2">Polymorphism in Clojure: A Tutorial Using Numbers, Part 2</a></li>
    
    <li><a href="/2016/11/polyclojure1">Polymorphism in Clojure: A Tutorial Using Numbers, Part 1</a></li>
    
    <li><a href="/2015/11/julia">Experiments with Julia</a></li>
    
  </ul>
</div>

<div class="sidebar">
  <h1>Tags</h1>
  <ul>
    
      <li><a href="/tag/julia">julia</a></li>
    
      <li><a href="/tag/c++">c++</a></li>
    
      <li><a href="/tag/simd">simd</a></li>
    
      <li><a href="/tag/clojure">clojure</a></li>
    
      <li><a href="/tag/polymorphism">polymorphism</a></li>
    
      <li><a href="/tag/functional-programming">functional-programming</a></li>
    
      <li><a href="/tag/python">python</a></li>
    
      <li><a href="/tag/machine-learning">machine-learning</a></li>
    
      <li><a href="/tag/computer-vision">computer-vision</a></li>
    
      <li><a href="/tag/opencv">opencv</a></li>
    
      <li><a href="/tag/bovw">bovw</a></li>
    
  </ul>
</div>

	  </div>
	  -->
      </div>
    </div>
  </div>
  <footer class="footer-distributed">
<div class="container">

	<div class="footer">
	<p><a href="/about">James Briggs</a> &copy; 2015-2017</p>
		<h6>Follow me</h6>

<ul class="social-media">

    
    <li>
        <a title="jimypbr on Github"
            href="https://github.com/jimypbr"
            target="_blank"><i class="fa fa-github fa-2x"></i></a>
    </li>
    

    
    <li>
        <a title="jimy-loon on StackOverflow"
			href="http://stackoverflow.com/users/jimy-loon"
            target="_blank"><i class="fa fa-stack-overflow fa-2x"></i></a>
    </li>
    

    
    <li>
        <a title="jimypbr on LinkedIn"
            href="https://www.linkedin.com/in/james-briggs-74229a65"
            target="_blank"><i class="fa fa-linkedin fa-2x"></i></a>
    </li>
    


    


    

    
    <li>
        <a title="feed.xml RSS"
            href="/feed.xml"
            target="_blank"><i class="fa fa-rss fa-2x"></i></a>
    </li>
    

</ul>

	</div>
	<div class="theme-origin">
		<p>Powered by <a href="https://jekyllrb.com/"> Jekyll</a>. Based on the theme <a href="https://github.com/streetturtle/jekyll-clean-dark">jekyll-clean-dark</a></p>.
</div>

</footer>

<!--
  <script type="text/javascript">
    var disqus_shortname = 'jimypbr-github-io';

   (function () {
     var s = document.createElement('script'); s.async = true;
     s.type = 'text/javascript';
     s.src = '//' + disqus_shortname + '.disqus.com/count.js';
     (document.getElementsByTagName('HEAD')[0] || document.getElementsByTagName('BODY')[0]).appendChild(s);
   }());
 </script>

 -->

  <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

    ga('create', '', 'auto');
    ga('send', 'pageview');
  </script>


</body>

</html>

</div>
